{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copyright @ Ziming Li\n",
    "# version 1.0\n",
    "# This code is created alone by Ziming Li, a Ph.D student from Tsinghua University, China.\n",
    "# This code is for the final project of the my summer internship in Yunzhixin'an Technology Co., LTD, Zhengzhou, China.\n",
    "# If you have any questions, please contact me by email: lzm22@mails.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(1229)\n",
    "\n",
    "from model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of word vector: 50\n",
      "length of sentence: 11\n",
      "number of labels: 22\n"
     ]
    }
   ],
   "source": [
    "# load data from ./data\n",
    "\n",
    "training_set = pickle.load(open('./data/training_set.pkl', 'rb'))\n",
    "training_labels = pickle.load(open('./data/training_labels.pkl', 'rb'))\n",
    "\n",
    "validation_set = pickle.load(open('./data/validation_set.pkl', 'rb'))\n",
    "validation_labels = pickle.load(open('./data/validation_labels.pkl', 'rb'))\n",
    "\n",
    "mapper = pickle.load(open('./data/mapper_between_index_and_label.pkl', 'rb'))\n",
    "\n",
    "length_of_word_vector = len(training_set[0][0])\n",
    "length_of_sentence = len(training_set[0])\n",
    "number_of_labels = len(mapper)\n",
    "\n",
    "print('length of word vector:', length_of_word_vector)\n",
    "print('length of sentence:', length_of_sentence)\n",
    "print('number of labels:', number_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "\n",
    "experiment_name = \"run\"\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "val_batch_size = 320\n",
    "number_of_classes = number_of_labels\n",
    "learning_rate = 1e-3\n",
    "embed_units = length_of_word_vector\n",
    "units = 64\n",
    "train_dir = './trained_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with fresh parameters.\n"
     ]
    }
   ],
   "source": [
    "# define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "\n",
    "print(\"Created model with fresh parameters.\")\n",
    "\n",
    "# model defination\n",
    "model = RNN(\n",
    "    embed_units, # wordvec size\n",
    "    units, # hidden size\n",
    "    number_of_classes) # number of words\n",
    "model.to(device)\n",
    "\n",
    "# optimizer defination\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# training preparation\n",
    "best_val_ppl = float(\"inf\")\n",
    "best_epoch = -1\n",
    "train_loss_record, valid_loss_record = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to DataLoader\n",
    "training_set = torch.from_numpy(np.array(training_set, dtype=np.float32))\n",
    "training_labels = torch.from_numpy(np.array(training_labels, dtype=np.int64))\n",
    "validation_set = torch.from_numpy(np.array(validation_set, dtype=np.float32))\n",
    "validation_labels = torch.from_numpy(np.array(validation_labels, dtype=np.int64))\n",
    "\n",
    "train_dataset = Data.TensorDataset(training_set, training_labels)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# for step,(batch_x,batch_y) in enumerate(train_loader):\n",
    "#     print('| Step: ', step, '| batch x: ', batch_x.numpy(), '| batch y: ', batch_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_sample(dataset, labels, batch_size):\n",
    "    indices = random.sample(range(len(labels)), batch_size)\n",
    "    return dataset[indices], labels[indices]\n",
    "\n",
    "# choose_random_sample(training_set, training_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 100, train loss 2.627065\n",
      "Epoch 1 Batch 200, train loss 1.469127\n",
      "Epoch 1 Batch 300, train loss 0.894586\n",
      "Epoch 1 Batch 400, train loss 0.610579\n",
      "Epoch 1 Batch 500, train loss 0.472886\n",
      "Epoch 1 Batch 600, train loss 0.399139\n",
      "Epoch 1 Batch 700, train loss 0.352792\n",
      "Epoch 1 Batch 800, train loss 0.360567\n",
      "Epoch 1 Batch 900, train loss 0.320150\n",
      "Epoch 1 Batch 1000, train loss 0.302620\n",
      "Epoch 1 Batch 1100, train loss 0.293878\n",
      "Epoch 1 Batch 1200, train loss 0.260569\n",
      "Epoch 1 Batch 1300, train loss 0.261530\n",
      "Epoch 1 Batch 1400, train loss 0.222316\n",
      "Epoch 1 Batch 1500, train loss 0.239415\n",
      "Epoch 1 Batch 1600, train loss 0.218006\n",
      "Epoch 1 Batch 1700, train loss 0.199954\n",
      "Epoch 1 Batch 1800, train loss 0.183520\n",
      "Epoch 1 Batch 1900, train loss 0.207169\n",
      "Epoch 1 Batch 2000, train loss 0.194234\n",
      "Epoch 1 Batch 2100, train loss 0.181548\n",
      "Epoch 1 Batch 2200, train loss 0.184535\n",
      "Epoch 1 Batch 2300, train loss 0.191008\n",
      "Epoch 1 Batch 2400, train loss 0.188694\n",
      "Epoch 1 Batch 2500, train loss 0.190797\n",
      "Epoch 1 Batch 2600, train loss 0.157844\n",
      "Epoch 1 Batch 2700, train loss 0.161316\n",
      "Epoch 1 Batch 2800, train loss 0.158492\n",
      "Epoch 1 Batch 2900, train loss 0.197885\n",
      "Epoch 1 Batch 3000, train loss 0.169521\n",
      "Epoch 1 Batch 3100, train loss 0.174931\n",
      "Epoch 1 Batch 3200, train loss 0.172347\n",
      "Epoch 1 Batch 3300, train loss 0.152301\n",
      "Epoch 1 Batch 3400, train loss 0.154854\n",
      "Epoch 1 Batch 3500, train loss 0.157800\n",
      "Epoch 1 Batch 3600, train loss 0.160215\n",
      "Epoch 1 Batch 3700, train loss 0.152357\n",
      "Epoch 1 Batch 3800, train loss 0.152614\n",
      "Epoch 1 Batch 3900, train loss 0.167468\n",
      "Epoch 1 Batch 4000, train loss 0.143850\n",
      "Epoch 1 Batch 4100, train loss 0.173068\n",
      "Epoch 1 Batch 4200, train loss 0.168374\n",
      "Epoch 1 Batch 4300, train loss 0.149980\n",
      "Epoch 1 Batch 4400, train loss 0.148923\n",
      "Epoch 1 Batch 4500, train loss 0.153761\n",
      "Epoch 1 Batch 4600, train loss 0.143504\n",
      "Epoch 1 Batch 4700, train loss 0.142955\n",
      "Epoch 1 Batch 4800, train loss 0.155284\n",
      "Epoch 1 Batch 4900, train loss 0.142307\n",
      "Epoch 1 Batch 5000, train loss 0.132680\n",
      "Epoch 1 Batch 5100, train loss 0.136072\n",
      "Epoch 1 Batch 5200, train loss 0.138750\n",
      "Epoch 1 Batch 5300, train loss 0.150484\n",
      "Epoch 1 Batch 5400, train loss 0.142634\n",
      "Epoch 1 Batch 5500, train loss 0.121279\n",
      "Epoch 1 of 5 took 44.044044733047485s\n",
      "  training loss:                 0.2847370246077295\n",
      "  validation correct percentage: 0.909375\n",
      "  best epoch:                    1\n",
      "  best validation perplexity:    0.909375\n",
      "Epoch 2 Batch 100, train loss 0.133395\n",
      "Epoch 2 Batch 200, train loss 0.136701\n",
      "Epoch 2 Batch 300, train loss 0.136840\n",
      "Epoch 2 Batch 400, train loss 0.133662\n",
      "Epoch 2 Batch 500, train loss 0.135223\n",
      "Epoch 2 Batch 600, train loss 0.127904\n",
      "Epoch 2 Batch 700, train loss 0.158508\n",
      "Epoch 2 Batch 800, train loss 0.126068\n",
      "Epoch 2 Batch 900, train loss 0.137290\n",
      "Epoch 2 Batch 1000, train loss 0.137906\n",
      "Epoch 2 Batch 1100, train loss 0.167231\n",
      "Epoch 2 Batch 1200, train loss 0.133110\n",
      "Epoch 2 Batch 1300, train loss 0.131825\n",
      "Epoch 2 Batch 1400, train loss 0.159290\n",
      "Epoch 2 Batch 1500, train loss 0.138246\n",
      "Epoch 2 Batch 1600, train loss 0.133336\n",
      "Epoch 2 Batch 1700, train loss 0.132416\n",
      "Epoch 2 Batch 1800, train loss 0.156336\n",
      "Epoch 2 Batch 1900, train loss 0.136516\n",
      "Epoch 2 Batch 2000, train loss 0.139373\n",
      "Epoch 2 Batch 2100, train loss 0.119611\n",
      "Epoch 2 Batch 2200, train loss 0.132734\n",
      "Epoch 2 Batch 2300, train loss 0.127848\n",
      "Epoch 2 Batch 2400, train loss 0.126926\n",
      "Epoch 2 Batch 2500, train loss 0.196200\n",
      "Epoch 2 Batch 2600, train loss 0.146395\n",
      "Epoch 2 Batch 2700, train loss 0.138277\n",
      "Epoch 2 Batch 2800, train loss 0.119445\n",
      "Epoch 2 Batch 2900, train loss 0.142030\n",
      "Epoch 2 Batch 3000, train loss 0.157669\n",
      "Epoch 2 Batch 3100, train loss 0.142279\n",
      "Epoch 2 Batch 3200, train loss 0.118776\n",
      "Epoch 2 Batch 3300, train loss 0.126252\n",
      "Epoch 2 Batch 3400, train loss 0.140156\n",
      "Epoch 2 Batch 3500, train loss 0.126333\n",
      "Epoch 2 Batch 3600, train loss 0.135489\n",
      "Epoch 2 Batch 3700, train loss 0.136670\n",
      "Epoch 2 Batch 3800, train loss 0.124302\n",
      "Epoch 2 Batch 3900, train loss 0.128474\n",
      "Epoch 2 Batch 4000, train loss 0.130904\n",
      "Epoch 2 Batch 4100, train loss 0.137640\n",
      "Epoch 2 Batch 4200, train loss 0.149352\n",
      "Epoch 2 Batch 4300, train loss 0.131288\n",
      "Epoch 2 Batch 4400, train loss 0.138192\n",
      "Epoch 2 Batch 4500, train loss 0.128541\n",
      "Epoch 2 Batch 4600, train loss 0.134971\n",
      "Epoch 2 Batch 4700, train loss 0.118717\n",
      "Epoch 2 Batch 4800, train loss 0.126941\n",
      "Epoch 2 Batch 4900, train loss 0.113974\n",
      "Epoch 2 Batch 5000, train loss 0.138203\n",
      "Epoch 2 Batch 5100, train loss 0.133524\n",
      "Epoch 2 Batch 5200, train loss 0.125338\n",
      "Epoch 2 Batch 5300, train loss 0.123373\n",
      "Epoch 2 Batch 5400, train loss 0.148166\n",
      "Epoch 2 Batch 5500, train loss 0.132410\n",
      "Epoch 2 of 5 took 45.28900599479675s\n",
      "  training loss:                 0.1361558961491947\n",
      "  validation correct percentage: 0.896875\n",
      "  best epoch:                    1\n",
      "  best validation perplexity:    0.909375\n",
      "Epoch 3 Batch 100, train loss 0.144851\n",
      "Epoch 3 Batch 200, train loss 0.125527\n",
      "Epoch 3 Batch 300, train loss 0.126962\n",
      "Epoch 3 Batch 400, train loss 0.119662\n",
      "Epoch 3 Batch 500, train loss 0.234585\n",
      "Epoch 3 Batch 600, train loss 0.121642\n",
      "Epoch 3 Batch 700, train loss 0.124745\n",
      "Epoch 3 Batch 800, train loss 0.129826\n",
      "Epoch 3 Batch 900, train loss 0.129904\n",
      "Epoch 3 Batch 1000, train loss 0.130058\n",
      "Epoch 3 Batch 1100, train loss 0.131830\n",
      "Epoch 3 Batch 1200, train loss 0.138055\n",
      "Epoch 3 Batch 1300, train loss 0.120797\n",
      "Epoch 3 Batch 1400, train loss 0.128830\n",
      "Epoch 3 Batch 1500, train loss 0.150301\n",
      "Epoch 3 Batch 1600, train loss 0.149762\n",
      "Epoch 3 Batch 1700, train loss 0.126271\n",
      "Epoch 3 Batch 1800, train loss 0.121454\n",
      "Epoch 3 Batch 1900, train loss 0.124127\n",
      "Epoch 3 Batch 2000, train loss 0.125217\n",
      "Epoch 3 Batch 2100, train loss 0.118494\n",
      "Epoch 3 Batch 2200, train loss 0.126365\n",
      "Epoch 3 Batch 2300, train loss 0.092885\n",
      "Epoch 3 Batch 2400, train loss 0.127663\n",
      "Epoch 3 Batch 2500, train loss 0.140023\n",
      "Epoch 3 Batch 2600, train loss 0.161038\n",
      "Epoch 3 Batch 2700, train loss 0.120211\n",
      "Epoch 3 Batch 2800, train loss 0.139298\n",
      "Epoch 3 Batch 2900, train loss 0.117323\n",
      "Epoch 3 Batch 3000, train loss 0.137102\n",
      "Epoch 3 Batch 3100, train loss 0.113376\n",
      "Epoch 3 Batch 3200, train loss 0.135560\n",
      "Epoch 3 Batch 3300, train loss 0.122050\n",
      "Epoch 3 Batch 3400, train loss 0.128880\n",
      "Epoch 3 Batch 3500, train loss 0.128287\n",
      "Epoch 3 Batch 3600, train loss 0.120472\n",
      "Epoch 3 Batch 3700, train loss 0.125133\n",
      "Epoch 3 Batch 3800, train loss 0.135031\n",
      "Epoch 3 Batch 3900, train loss 0.120453\n",
      "Epoch 3 Batch 4000, train loss 0.130047\n",
      "Epoch 3 Batch 4100, train loss 0.130069\n",
      "Epoch 3 Batch 4200, train loss 0.114947\n",
      "Epoch 3 Batch 4300, train loss 0.115370\n",
      "Epoch 3 Batch 4400, train loss 0.134755\n",
      "Epoch 3 Batch 4500, train loss 0.120766\n",
      "Epoch 3 Batch 4600, train loss 0.137737\n",
      "Epoch 3 Batch 4700, train loss 0.253091\n",
      "Epoch 3 Batch 4800, train loss 0.120378\n",
      "Epoch 3 Batch 4900, train loss 0.147170\n",
      "Epoch 3 Batch 5000, train loss 0.139305\n",
      "Epoch 3 Batch 5100, train loss 0.121009\n",
      "Epoch 3 Batch 5200, train loss 0.140417\n",
      "Epoch 3 Batch 5300, train loss 0.127199\n",
      "Epoch 3 Batch 5400, train loss 0.134125\n",
      "Epoch 3 Batch 5500, train loss 0.125456\n",
      "Epoch 3 of 5 took 43.75248408317566s\n",
      "  training loss:                 0.13283443369824355\n",
      "  validation correct percentage: 0.903125\n",
      "  best epoch:                    1\n",
      "  best validation perplexity:    0.909375\n",
      "Epoch 4 Batch 100, train loss 0.119388\n",
      "Epoch 4 Batch 200, train loss 0.125770\n",
      "Epoch 4 Batch 300, train loss 0.133731\n",
      "Epoch 4 Batch 400, train loss 0.120355\n",
      "Epoch 4 Batch 500, train loss 0.137706\n",
      "Epoch 4 Batch 600, train loss 0.137358\n",
      "Epoch 4 Batch 700, train loss 0.114207\n",
      "Epoch 4 Batch 800, train loss 0.124380\n",
      "Epoch 4 Batch 900, train loss 0.117637\n",
      "Epoch 4 Batch 1000, train loss 0.130816\n",
      "Epoch 4 Batch 1100, train loss 0.126591\n",
      "Epoch 4 Batch 1200, train loss 0.132605\n",
      "Epoch 4 Batch 1300, train loss 0.113567\n",
      "Epoch 4 Batch 1400, train loss 0.121500\n",
      "Epoch 4 Batch 1500, train loss 0.127280\n",
      "Epoch 4 Batch 1600, train loss 0.126590\n",
      "Epoch 4 Batch 1700, train loss 0.126683\n",
      "Epoch 4 Batch 1800, train loss 0.130734\n",
      "Epoch 4 Batch 1900, train loss 0.125480\n",
      "Epoch 4 Batch 2000, train loss 0.129232\n",
      "Epoch 4 Batch 2100, train loss 0.112648\n",
      "Epoch 4 Batch 2200, train loss 0.117839\n",
      "Epoch 4 Batch 2300, train loss 0.123720\n",
      "Epoch 4 Batch 2400, train loss 0.118863\n",
      "Epoch 4 Batch 2500, train loss 0.125167\n",
      "Epoch 4 Batch 2600, train loss 0.124232\n",
      "Epoch 4 Batch 2700, train loss 0.125660\n",
      "Epoch 4 Batch 2800, train loss 0.118510\n",
      "Epoch 4 Batch 2900, train loss 0.134327\n",
      "Epoch 4 Batch 3000, train loss 0.125860\n",
      "Epoch 4 Batch 3100, train loss 0.147355\n",
      "Epoch 4 Batch 3200, train loss 0.121977\n",
      "Epoch 4 Batch 3300, train loss 0.140904\n",
      "Epoch 4 Batch 3400, train loss 0.134272\n",
      "Epoch 4 Batch 3500, train loss 0.128235\n",
      "Epoch 4 Batch 3600, train loss 0.123589\n",
      "Epoch 4 Batch 3700, train loss 0.122812\n",
      "Epoch 4 Batch 3800, train loss 0.115160\n",
      "Epoch 4 Batch 3900, train loss 0.122052\n",
      "Epoch 4 Batch 4000, train loss 0.131222\n",
      "Epoch 4 Batch 4100, train loss 0.116748\n",
      "Epoch 4 Batch 4200, train loss 0.132434\n",
      "Epoch 4 Batch 4300, train loss 0.112954\n",
      "Epoch 4 Batch 4400, train loss 0.127783\n",
      "Epoch 4 Batch 4500, train loss 0.134676\n",
      "Epoch 4 Batch 4600, train loss 0.118627\n",
      "Epoch 4 Batch 4700, train loss 0.109817\n",
      "Epoch 4 Batch 4800, train loss 0.128884\n",
      "Epoch 4 Batch 4900, train loss 0.117360\n",
      "Epoch 4 Batch 5000, train loss 0.156729\n",
      "Epoch 4 Batch 5100, train loss 0.132297\n",
      "Epoch 4 Batch 5200, train loss 0.128757\n",
      "Epoch 4 Batch 5300, train loss 0.120661\n",
      "Epoch 4 Batch 5400, train loss 0.117559\n",
      "Epoch 4 Batch 5500, train loss 0.117385\n",
      "Epoch 4 of 5 took 43.27702188491821s\n",
      "  training loss:                 0.1256124799176204\n",
      "  validation correct percentage: 0.934375\n",
      "  best epoch:                    4\n",
      "  best validation perplexity:    0.934375\n",
      "Epoch 5 Batch 100, train loss 0.132188\n",
      "Epoch 5 Batch 200, train loss 0.123784\n",
      "Epoch 5 Batch 300, train loss 0.109908\n",
      "Epoch 5 Batch 400, train loss 0.114991\n",
      "Epoch 5 Batch 500, train loss 0.126536\n",
      "Epoch 5 Batch 600, train loss 0.120208\n",
      "Epoch 5 Batch 700, train loss 0.141891\n",
      "Epoch 5 Batch 800, train loss 0.123441\n",
      "Epoch 5 Batch 900, train loss 0.123815\n",
      "Epoch 5 Batch 1000, train loss 0.115204\n",
      "Epoch 5 Batch 1100, train loss 0.122922\n",
      "Epoch 5 Batch 1200, train loss 0.110553\n",
      "Epoch 5 Batch 1300, train loss 0.115664\n",
      "Epoch 5 Batch 1400, train loss 0.125640\n",
      "Epoch 5 Batch 1500, train loss 0.120800\n",
      "Epoch 5 Batch 1600, train loss 0.118959\n",
      "Epoch 5 Batch 1700, train loss 0.112486\n",
      "Epoch 5 Batch 1800, train loss 0.144233\n",
      "Epoch 5 Batch 1900, train loss 0.126349\n",
      "Epoch 5 Batch 2000, train loss 0.121844\n",
      "Epoch 5 Batch 2100, train loss 0.114860\n",
      "Epoch 5 Batch 2200, train loss 0.142830\n",
      "Epoch 5 Batch 2300, train loss 0.135431\n",
      "Epoch 5 Batch 2400, train loss 0.131035\n",
      "Epoch 5 Batch 2500, train loss 0.129514\n",
      "Epoch 5 Batch 2600, train loss 0.121015\n",
      "Epoch 5 Batch 2700, train loss 0.099559\n",
      "Epoch 5 Batch 2800, train loss 0.112071\n",
      "Epoch 5 Batch 2900, train loss 0.124796\n",
      "Epoch 5 Batch 3000, train loss 0.136430\n",
      "Epoch 5 Batch 3100, train loss 0.133736\n",
      "Epoch 5 Batch 3200, train loss 0.108976\n",
      "Epoch 5 Batch 3300, train loss 0.121778\n",
      "Epoch 5 Batch 3400, train loss 0.119649\n",
      "Epoch 5 Batch 3500, train loss 0.121460\n",
      "Epoch 5 Batch 3600, train loss 0.126851\n",
      "Epoch 5 Batch 3700, train loss 0.131240\n",
      "Epoch 5 Batch 3800, train loss 0.117390\n",
      "Epoch 5 Batch 3900, train loss 0.119223\n",
      "Epoch 5 Batch 4000, train loss 0.134868\n",
      "Epoch 5 Batch 4100, train loss 0.109883\n",
      "Epoch 5 Batch 4200, train loss 0.120840\n",
      "Epoch 5 Batch 4300, train loss 0.126426\n",
      "Epoch 5 Batch 4400, train loss 0.133981\n",
      "Epoch 5 Batch 4500, train loss 0.115407\n",
      "Epoch 5 Batch 4600, train loss 0.115055\n",
      "Epoch 5 Batch 4700, train loss 0.124696\n",
      "Epoch 5 Batch 4800, train loss 0.135583\n",
      "Epoch 5 Batch 4900, train loss 0.137604\n",
      "Epoch 5 Batch 5000, train loss 0.133015\n",
      "Epoch 5 Batch 5100, train loss 0.124569\n",
      "Epoch 5 Batch 5200, train loss 0.132866\n",
      "Epoch 5 Batch 5300, train loss 0.106163\n",
      "Epoch 5 Batch 5400, train loss 0.108696\n",
      "Epoch 5 Batch 5500, train loss 0.132650\n",
      "Epoch 5 of 5 took 43.04552102088928s\n",
      "  training loss:                 0.12348286300484854\n",
      "  validation correct percentage: 0.890625\n",
      "  best epoch:                    4\n",
      "  best validation perplexity:    0.934375\n"
     ]
    }
   ],
   "source": [
    "# begin training\n",
    "best_epoch = -1\n",
    "best_val_correct_percentage = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (batched_data, batched_label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = model(batched_data, batched_label, device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.tolist())\n",
    "\n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(\"Epoch %d Batch %d, train loss %f\" % (epoch, batch + 1, np.mean(losses[-100:])))\n",
    "\n",
    "    train_loss = np.mean(losses)\n",
    "    train_loss_record.append(train_loss)\n",
    "\n",
    "    validation_batched_data, validation_batched_label = choose_random_sample(validation_set, validation_labels, val_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_ans = model.predict(validation_batched_data, device)\n",
    "        val_correct_percentage = torch.sum(predicted_ans == validation_batched_label).item() / val_batch_size\n",
    "\n",
    "        if val_correct_percentage > best_val_correct_percentage:\n",
    "            best_val_correct_percentage = val_correct_percentage\n",
    "            best_epoch = epoch\n",
    "\n",
    "            with open(os.path.join(train_dir, 'checkpoint_%s.pth' % experiment_name), 'wb') as fout:\n",
    "                torch.save(model, fout)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(\"Epoch \" + str(epoch) + \" of \" + str(num_epochs) + \" took \" + str(epoch_time) + \"s\")\n",
    "    print(\"  training loss:                 \" + str(train_loss))\n",
    "    print(\"  validation correct percentage: \" + str(val_correct_percentage))\n",
    "    print(\"  best epoch:                    \" + str(best_epoch))\n",
    "    print(\"  best validation perplexity:    \" + str(best_val_correct_percentage))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
