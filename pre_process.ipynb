{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copyright @ Ziming Li\n",
    "# version 1.0\n",
    "# This code is created alone by Ziming Li, a Ph.D student from Tsinghua University, China.\n",
    "# This code is for the final project of the my summer internship in Yunzhixin'an Technology Co., LTD, Zhengzhou, China.\n",
    "# If you have any questions, please contact me by email: lzm22@mails.tsinghua.edu.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vector_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this file: word2vec and establish the data set for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sentences = [] # sentences from the file to establish the word2vec model\n",
    "raw_strings = [] # raw strings from the file to be classified\n",
    "data_set_sentences = [] # sentences from the data set to be classified, preseve the word order\n",
    "class_labels = [] # class labels for the data set\n",
    "split_chars = [' ', '-', '+', '(', ')', '/', '*', ',', '.', ':', '@', '#', '_', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file, path = './raw_training_data.csv'\n",
    "# read each column, append to sentences\n",
    "\n",
    "with open('./raw_training_data.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    # get the header\n",
    "    header = next(reader)\n",
    "    column_number = len(header)\n",
    "\n",
    "    for i in range(column_number):\n",
    "        file_sentences.append([])\n",
    "    for row in reader:\n",
    "\n",
    "        for i in range(column_number):\n",
    "            \n",
    "            raw_strings.append(row[i])\n",
    "            tmp_sentence = row[i].lower()\n",
    "\n",
    "            # split sentence into words\n",
    "            for char in split_chars:\n",
    "                tmp_sentence = tmp_sentence.replace(char, ',')\n",
    "            words = tmp_sentence.split(',')\n",
    "            words = [word for word in words if word != '']\n",
    "\n",
    "            # add to sentences recorder\n",
    "            data_set_sentences.append(words)\n",
    "            class_labels.append(i)\n",
    "            for word in words:\n",
    "                file_sentences[i].append(word)\n",
    "\n",
    "# randomize the order of the sentences and the labels, but keep the matching between them\n",
    "import random\n",
    "random.seed(0)\n",
    "random.shuffle(raw_strings)\n",
    "random.seed(0)\n",
    "random.shuffle(data_set_sentences)\n",
    "random.seed(0)\n",
    "random.shuffle(class_labels)\n",
    "\n",
    "# print('Data set size:', len(data_set_sentences))\n",
    "# print('Class labels size:', len(class_labels))\n",
    "# print(data_set_sentences[:10])\n",
    "# print(class_labels[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### refer to word2vec model: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "### establish the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = Word2Vec(\n",
    "    sentences = file_sentences,\n",
    "    vector_size = vector_size,\n",
    "    min_count = 1,\n",
    "    window = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for kelly:\n",
      "[ 0.86695564 -0.28123128  0.73866993  0.7363451  -0.863093    0.2048312\n",
      "  0.0258419  -0.66088855 -0.17692803 -0.7492832   0.34000912  0.06237699\n",
      "  0.31536603 -0.11916703  0.22222662 -0.05943888  1.1584444   0.20450635\n",
      " -0.7332263  -1.0642958  -0.07563085  0.50791556  0.5548607   0.21765712\n",
      " -0.0282851   0.6083949  -0.4134535   1.0337824  -0.05319791  0.2527455\n",
      " -0.6261593  -0.4706853   0.60876626 -0.76131266 -0.7357941  -0.59791815\n",
      " -0.26468563  0.05810263 -0.7626965  -0.21389204  0.4078369  -0.08348862\n",
      "  0.19382037 -0.24699345  0.7604668  -0.45386875 -0.10818419 -0.6490807\n",
      "  0.7058431   0.4393891 ]\n",
      "successfully encode words into vectors\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector for kelly:\")\n",
    "print(lang_model.wv['kelly'])\n",
    "print('successfully encode words into vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_length: 11\n"
     ]
    }
   ],
   "source": [
    "sentence_length = 0\n",
    "for sentence in data_set_sentences:\n",
    "    sentence_length = max(sentence_length, len(sentence))\n",
    "\n",
    "print('sentence_length:', sentence_length)\n",
    "\n",
    "# pad the sentences to the same length\n",
    "\n",
    "data_set_vectors = [[] for _ in range(len(data_set_sentences))] # turn data_set_sentences into vectors\n",
    "for i in range(len(data_set_sentences)):\n",
    "    for word in data_set_sentences[i]:\n",
    "        data_set_vectors[i].append(lang_model.wv[word])\n",
    "    while len(data_set_vectors[i]) < sentence_length:\n",
    "        data_set_vectors[i].append([0 for _ in range(vector_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 176000\n",
      "Validation set size: 22000\n",
      "Test set size: 22000\n"
     ]
    }
   ],
   "source": [
    "# divide data set into training set, validation set and test set\n",
    "training_set_size = int(len(data_set_vectors) * 0.8)\n",
    "validation_set_size = int(len(data_set_vectors) * 0.1)\n",
    "test_set_size = len(data_set_vectors) - training_set_size - validation_set_size\n",
    "\n",
    "training_set = data_set_vectors[:training_set_size]\n",
    "training_labels = class_labels[:training_set_size]\n",
    "\n",
    "validation_set = data_set_vectors[training_set_size:training_set_size + validation_set_size]\n",
    "validation_labels = class_labels[training_set_size:training_set_size + validation_set_size]\n",
    "\n",
    "raw_test_strings = raw_strings[training_set_size + validation_set_size:]\n",
    "test_set = data_set_vectors[training_set_size + validation_set_size:]\n",
    "test_labels = class_labels[training_set_size + validation_set_size:]\n",
    "\n",
    "print('Training set size:', len(training_set))\n",
    "print('Validation set size:', len(validation_set))\n",
    "print('Test set size:', len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data set as python list type\n",
    "\n",
    "import pickle\n",
    "with open('./data/training_set.pkl', 'wb') as f:\n",
    "    pickle.dump(training_set, f)\n",
    "with open('./data/training_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(training_labels, f)\n",
    "\n",
    "with open('./data/validation_set.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_set, f)\n",
    "with open('./data/validation_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(validation_labels, f)\n",
    "\n",
    "with open('./data/raw_test_strings.pkl', 'wb') as f:\n",
    "    pickle.dump(raw_test_strings, f)\n",
    "with open('./data/test_set.pkl', 'wb') as f:\n",
    "    pickle.dump(test_set, f)\n",
    "with open('./data/test_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)\n",
    "\n",
    "with open('./data/mapper_between_index_and_label.pkl', 'wb') as f:\n",
    "    pickle.dump(header, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
